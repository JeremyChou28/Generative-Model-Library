{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacent_matrix(distance_file: str,\n",
    "                        num_nodes: int,\n",
    "                        id_file: str = None,\n",
    "                        graph_type=\"connect\") -> np.array:\n",
    "    \"\"\"\n",
    "    construct adjacent matrix by csv file   根据PEMS数据集的csv文件来构建邻接矩阵\n",
    "    :param distance_file: path of csv file to save the distances between nodes  # csv文件路径\n",
    "    :param num_nodes: number of nodes in the graph   graph中节点个数\n",
    "    :param id_file: path of txt file to save the order of the nodes     \n",
    "    :param graph_type: [\"connect\", \"distance\"] if use weight, please set distance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    A = np.zeros([int(num_nodes), int(num_nodes)])\n",
    "\n",
    "    if id_file:\n",
    "        with open(id_file, \"r\") as f_id:\n",
    "            node_id_dict = {\n",
    "                int(node_id): idx\n",
    "                for idx, node_id in enumerate(f_id.read().strip().split(\"\\n\"))\n",
    "            }\n",
    "\n",
    "            with open(distance_file, \"r\") as f_d:\n",
    "                f_d.readline()\n",
    "                reader = csv.reader(f_d)\n",
    "                for item in reader:\n",
    "                    if len(item) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "                    if graph_type == \"connect\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1.\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1.\n",
    "                    elif graph_type == \"distance\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1. / distance\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1. / distance\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"graph type is not correct (connect or distance)\")\n",
    "        return A\n",
    "\n",
    "    with open(distance_file, \"r\") as f_d:\n",
    "        f_d.readline()\n",
    "        reader = csv.reader(f_d)\n",
    "        for item in reader:\n",
    "            if len(item) != 3:\n",
    "                continue\n",
    "            i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "\n",
    "            if graph_type == \"connect\":\n",
    "                A[i, j], A[j, i] = 1., 1.\n",
    "            elif graph_type == \"distance\":\n",
    "                A[i, j] = 1. / distance\n",
    "                A[j, i] = 1. / distance\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"graph type is not correct (connect or distance)\")\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def get_flow_data(flow_file: str) -> np.array:\n",
    "    \"\"\"\n",
    "    parse npz to get flow data  读取npz文件得到交通流数据\n",
    "    :param flow_file: (N, T, D)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = np.load(flow_file)\n",
    "    flow_data = data['data'].transpose([1, 0,\n",
    "                                        2])[:, :,\n",
    "                                            0][:, :,\n",
    "                                               np.newaxis]  # [N, T, D]  D = 1\n",
    "    return flow_data\n",
    "\n",
    "\n",
    "class PEMSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, num_nodes, divide_days, time_interval,\n",
    "                 history_length, train_mode):\n",
    "        \"\"\"\n",
    "        load processed data\n",
    "        :param data_path: [\"graph file name\" , \"flow data file name\"], path to save the data file names\n",
    "        :param num_nodes: number of nodes in graph\n",
    "        :param divide_days: [ days of train data, days of test data], list to divide the original data\n",
    "        :param time_interval: time interval between two traffic data records (mins)\n",
    "        :param history_length: length of history data to be used\n",
    "        :param train_mode: [\"train\", \"test\"]\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.num_nodes = num_nodes\n",
    "        self.train_mode = train_mode\n",
    "        self.train_days = divide_days[0]\n",
    "        self.test_days = divide_days[1]\n",
    "        self.history_length = history_length  # 6\n",
    "        self.time_interval = time_interval  # 5 min\n",
    "        self.one_day_length = int(24 * 60 / self.time_interval)\n",
    "        self.graph = get_adjacent_matrix(distance_file=data_path[0],\n",
    "                                         num_nodes=num_nodes)\n",
    "        self.flow_norm, self.flow_data = self.pre_process_data(\n",
    "            data=get_flow_data(data_path[1]), norm_dim=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train_mode == \"train\":\n",
    "            # return self.train_days * self.one_day_length - self.history_length  # 这里为什么要减掉一个history length，导致最后训练数据的长度为45*24*12-6=12954\n",
    "            return self.train_days * self.one_day_length  # 这里为什么要减掉一个history length，导致最后训练数据的长度为45*24*12-6=12954\n",
    "        elif self.train_mode == \"test\":\n",
    "            return self.test_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(\n",
    "                self.train_mode))\n",
    "\n",
    "    def __getitem__(self, index):  # (x, y), index = [0, L1 - 1]\n",
    "        if self.train_mode == \"train\":\n",
    "            index = index\n",
    "        elif self.train_mode == \"test\":\n",
    "            index += self.train_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(\n",
    "                self.train_mode))\n",
    "\n",
    "        data_x, data_y = PEMSDataset.slice_data(self.flow_data,\n",
    "                                                self.history_length, index,\n",
    "                                                self.train_mode)\n",
    "        data_x = PEMSDataset.to_tensor(data_x)  # [N, H, D]\n",
    "        data_y = PEMSDataset.to_tensor(data_y).unsqueeze(1)  # [N, 1, D]\n",
    "        return {\n",
    "            \"graph\": PEMSDataset.to_tensor(self.graph),\n",
    "            \"flow_x\": data_x,\n",
    "            \"flow_y\": data_y\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def slice_data(data, history_length, index, train_mode):\n",
    "        \"\"\"\n",
    "        :param data: np.array, normalized traffic data.\n",
    "        :param history_length: int, length of history data to be used.\n",
    "        :param index: int, index on temporal axis.\n",
    "        :param train_mode: str, [\"train\", \"test\"].\n",
    "        :return:\n",
    "            data_x: np.array, [N, H, D].\n",
    "            data_y: np.array [N, D].\n",
    "        \"\"\"\n",
    "        if train_mode == \"train\":\n",
    "            start_index = index\n",
    "            end_index = index + history_length\n",
    "        elif train_mode == \"test\":\n",
    "            start_index = index - history_length\n",
    "            end_index = index\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"train model {} is not defined\".format(train_mode))\n",
    "\n",
    "        data_x = data[:, start_index:end_index]\n",
    "        data_y = data[:, end_index]\n",
    "\n",
    "        return data_x, data_y\n",
    "\n",
    "    @staticmethod\n",
    "    def pre_process_data(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            norm_base: list, [max_data, min_data], data of normalization base.\n",
    "            norm_data: np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        norm_base = PEMSDataset.normalize_base(\n",
    "            data, norm_dim)  # find the normalize base\n",
    "        norm_data = PEMSDataset.normalize_data(norm_base[0], norm_base[1],\n",
    "                                               data)  # normalize data\n",
    "\n",
    "        return norm_base, norm_data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_base(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            max_data: np.array\n",
    "            min_data: np.array\n",
    "        \"\"\"\n",
    "        max_data = np.max(data, norm_dim,\n",
    "                          keepdims=True)  # [N, T, D] , norm_dim=1, [N, 1, D]\n",
    "        min_data = np.min(data, norm_dim, keepdims=True)\n",
    "        return max_data, min_data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :return:\n",
    "            np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "        normalized_data = (data - mid) / base\n",
    "\n",
    "        return normalized_data\n",
    "\n",
    "    @staticmethod\n",
    "    def recover_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, normalized data.\n",
    "        :return:\n",
    "            recovered_data: np.array, recovered data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "\n",
    "        recovered_data = data * base + mid\n",
    "\n",
    "        return recovered_data\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensor(data):\n",
    "        return torch.tensor(data, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow(filename):\n",
    "    flow_data = np.load(filename)\n",
    "    # print(type(flow_data))\n",
    "    # PEMS数据集的data.npz文件中的data这个文件存放数据\n",
    "    return flow_data['data']\n",
    "\n",
    "def read_dataset(dataset):\n",
    "    data = np.load(dataset)\n",
    "    print(type(data['data']))\n",
    "    print(data['data'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307, 16992, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "data size (16992, 307, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(307, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset_file='/home/zhoujianping/Research/ASTGNN/data/PEMS04/PEMS04.npz'\n",
    "traffic_flow_data=get_flow_data(dataset_file)   \n",
    "print(traffic_flow_data.shape)  # (307,16992,1)只取flow这个维度，对tensor做了transform\n",
    "print(type(traffic_flow_data))\n",
    "\n",
    "\n",
    "traffic_data = get_flow(dataset_file)\n",
    "print(\"data size {}\".format(traffic_data.shape))    # (16992,307,3)其中16992代表时间长度 59天*24小时*12次（每5分钟采集一次数据），307是指检测器数量，3是指数据维度分别对应flow，occupy，speed\n",
    "\n",
    "read_dataset(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.PEMSDataset'>\n",
      "12960\n",
      "<class 'dict'>\n",
      "torch.Size([307, 307])\n",
      "torch.Size([307, 6, 1])\n",
      "torch.Size([307, 1, 1])\n",
      "4032\n"
     ]
    }
   ],
   "source": [
    "def get_loader(ds_name=\"PEMS04\"):\n",
    "    num_nodes = 307 if ds_name == 'PEMS04' else 170\n",
    "    train_data = PEMSDataset(data_path=[\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/distance.csv\".format(ds_name),\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/data.npz\".format(ds_name)\n",
    "    ],\n",
    "                             num_nodes=num_nodes,\n",
    "                             divide_days=[45, 14],\n",
    "                             time_interval=5,\n",
    "                             history_length=6,\n",
    "                             train_mode=\"train\")\n",
    "\n",
    "    print(type(train_data))\n",
    "    print(len(train_data))\n",
    "    print(type(train_data[0]))\n",
    "    print(train_data[0]['graph'].shape)\n",
    "    print(train_data[0]['flow_x'].shape)\n",
    "    print(train_data[0]['flow_y'].shape)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_data = PEMSDataset(data_path=[\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/distance.csv\".format(ds_name),\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/data.npz\".format(ds_name)\n",
    "    ],\n",
    "                            num_nodes=num_nodes,\n",
    "                            divide_days=[45, 14],\n",
    "                            time_interval=5,\n",
    "                            history_length=6,\n",
    "                            train_mode=\"test\")\n",
    "\n",
    "    print(len(test_data))\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "    \n",
    "train_loader, test_loader = get_loader('PEMS04')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "63\n",
      "<class 'dict'>\n",
      "dict_keys(['graph', 'flow_x', 'flow_y'])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 307, 307])\n",
      "torch.Size([64, 307, 6, 1])\n",
      "torch.Size([64, 307, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))    # math.ceil(45*24*12/64)=203，原始数据集的前45天数据作为训练集，后面14天作为测试集\n",
    "print(len(test_loader))\n",
    "for data in train_loader:   # batch_size=64\n",
    "    # print(data)\n",
    "    print(type(data))\n",
    "    print(data.keys())\n",
    "    print(type(data['graph']))\n",
    "    print(type(data['flow_x']))\n",
    "    print(type(data['flow_y']))\n",
    "    print(data['graph'].shape)\n",
    "    print(data['flow_x'].shape) # flow_x表示history data，长度取6个\n",
    "    print(data['flow_y'].shape) # flow_y表示要预测的值\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 307, 6, 1])\n",
      "torch.Size([64, 307, 6])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data['flow_x'].shape)\n",
    "    print(torch.squeeze(data['flow_x'],dim=3).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 调用父类方法初始化模块的state\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # 编码器 ： [b, 784] => [b, 20]\n",
    "        self.encoder = nn.Sequential(nn.Linear(1842, 256), nn.ReLU(),\n",
    "                                     nn.Linear(256, 20), nn.ReLU())\n",
    "\n",
    "        # 解码器 ： [b, 20] => [b, 784]\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1842),\n",
    "            nn.Sigmoid()  # 图片数值取值为[0,1]，不宜用ReLU\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        向前传播部分, 在model_name(inputs)时自动调用\n",
    "        :param x: the input of our training model\n",
    "        :return: the result of our training model\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]  # 每一批含有的样本的个数\n",
    "        # flatten\n",
    "        # tensor.view()方法可以调整tensor的形状，但必须保证调整前后元素总数一致。view不会修改自身的数据，\n",
    "        # 返回的新tensor与原tensor共享内存，即更改一个，另一个也随之改变。\n",
    "        x = x.view(batch_size, 1842)  # 一行代表一个样本\n",
    "\n",
    "        # encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        # reshape\n",
    "        x = x.view(batch_size, 307, 6)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 设备配置\n",
    "torch.cuda.set_device(0) # 这句用来设置pytorch在哪块GPU上运行\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-3    # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The structure of our model is shown below: \n",
      "\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1842, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1842, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "AEmodel = AE().to(device)  # 生成AE模型，并转移到GPU上去\n",
    "print('The structure of our model is shown below: \\n')\n",
    "print(AEmodel)\n",
    "loss_function = nn.MSELoss()  # 生成损失函数\n",
    "optimizer = optim.Adam(AEmodel.parameters(),\n",
    "                        lr=learning_rate)  # 生成优化器，需要优化的是model的参数，学习率为0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] :  loss =  0.0058744982816278934\n",
      "Epoch [1/10] :  loss =  0.004341111984103918\n",
      "Epoch [2/10] :  loss =  0.004707020707428455\n",
      "Epoch [3/10] :  loss =  0.004543520975857973\n",
      "Epoch [4/10] :  loss =  0.0037450729869306087\n",
      "Epoch [5/10] :  loss =  0.0037430343218147755\n",
      "Epoch [6/10] :  loss =  0.004415844101458788\n",
      "Epoch [7/10] :  loss =  0.004109341185539961\n",
      "Epoch [8/10] :  loss =  0.004412881564348936\n",
      "Epoch [9/10] :  loss =  0.0037341953720897436\n"
     ]
    }
   ],
   "source": [
    "# 开始迭代\n",
    "num_epochs=10\n",
    "loss_epoch = []\n",
    "for epoch in range(num_epochs):\n",
    "    # 每一代都要遍历所有的批次\n",
    "    for data in train_loader:\n",
    "        x=torch.squeeze(data['flow_x'],dim=3)\n",
    "        # [64,307,6]\n",
    "        x = x.to(device)\n",
    "        # 前向传播\n",
    "        x_hat = AEmodel(x)  # 模型的输出，在这里会自动调用model中的forward函数\n",
    "        loss = loss_function(x_hat, x)  # 计算损失值，即目标函数\n",
    "        # 后向传播\n",
    "        optimizer.zero_grad()  # 梯度清零，否则上一步的梯度仍会存在\n",
    "        loss.backward()  # 后向传播计算梯度，这些梯度会保存在model.parameters里面\n",
    "        optimizer.step()  # 更新梯度，这一步与上一步主要是根据model.parameters联系起来了\n",
    "\n",
    "    loss_epoch.append(loss.item())\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print('Epoch [{}/{}] : '.format(epoch, num_epochs), 'loss = ',\n",
    "                loss.item())  # loss是Tensor类型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch3090",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e578467fa5c12cffc301a3bc3421e1911b67151edde074c28fc0dd02d3ed613c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
