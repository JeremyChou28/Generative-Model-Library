{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacent_matrix(distance_file: str,\n",
    "                        num_nodes: int,\n",
    "                        id_file: str = None,\n",
    "                        graph_type=\"connect\") -> np.array:\n",
    "    \"\"\"\n",
    "    construct adjacent matrix by csv file   根据PEMS数据集的csv文件来构建邻接矩阵\n",
    "    :param distance_file: path of csv file to save the distances between nodes  # csv文件路径\n",
    "    :param num_nodes: number of nodes in the graph   graph中节点个数\n",
    "    :param id_file: path of txt file to save the order of the nodes     \n",
    "    :param graph_type: [\"connect\", \"distance\"] if use weight, please set distance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    A = np.zeros([int(num_nodes), int(num_nodes)])\n",
    "\n",
    "    if id_file:\n",
    "        with open(id_file, \"r\") as f_id:\n",
    "            node_id_dict = {\n",
    "                int(node_id): idx\n",
    "                for idx, node_id in enumerate(f_id.read().strip().split(\"\\n\"))\n",
    "            }\n",
    "\n",
    "            with open(distance_file, \"r\") as f_d:\n",
    "                f_d.readline()\n",
    "                reader = csv.reader(f_d)\n",
    "                for item in reader:\n",
    "                    if len(item) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "                    if graph_type == \"connect\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1.\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1.\n",
    "                    elif graph_type == \"distance\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1. / distance\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1. / distance\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"graph type is not correct (connect or distance)\")\n",
    "        return A\n",
    "\n",
    "    with open(distance_file, \"r\") as f_d:\n",
    "        f_d.readline()\n",
    "        reader = csv.reader(f_d)\n",
    "        for item in reader:\n",
    "            if len(item) != 3:\n",
    "                continue\n",
    "            i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "\n",
    "            if graph_type == \"connect\":\n",
    "                A[i, j], A[j, i] = 1., 1.\n",
    "            elif graph_type == \"distance\":\n",
    "                A[i, j] = 1. / distance\n",
    "                A[j, i] = 1. / distance\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"graph type is not correct (connect or distance)\")\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def get_flow_data(flow_file: str) -> np.array:\n",
    "    \"\"\"\n",
    "    parse npz to get flow data  读取npz文件得到交通流数据\n",
    "    :param flow_file: (N, T, D)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = np.load(flow_file)\n",
    "    flow_data = data['data'].transpose([1, 0,\n",
    "                                        2])[:, :,\n",
    "                                            0][:, :,\n",
    "                                               np.newaxis]  # [N, T, D]  D = 1\n",
    "    return flow_data\n",
    "\n",
    "\n",
    "class PEMSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, num_nodes, divide_days, time_interval,\n",
    "                 history_length, train_mode):\n",
    "        \"\"\"\n",
    "        load processed data\n",
    "        :param data_path: [\"graph file name\" , \"flow data file name\"], path to save the data file names\n",
    "        :param num_nodes: number of nodes in graph\n",
    "        :param divide_days: [ days of train data, days of test data], list to divide the original data\n",
    "        :param time_interval: time interval between two traffic data records (mins)\n",
    "        :param history_length: length of history data to be used\n",
    "        :param train_mode: [\"train\", \"test\"]\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.num_nodes = num_nodes\n",
    "        self.train_mode = train_mode\n",
    "        self.train_days = divide_days[0]\n",
    "        self.test_days = divide_days[1]\n",
    "        self.history_length = history_length  # 6\n",
    "        self.time_interval = time_interval  # 5 min\n",
    "        self.one_day_length = int(24 * 60 / self.time_interval)\n",
    "        self.graph = get_adjacent_matrix(distance_file=data_path[0],\n",
    "                                         num_nodes=num_nodes)\n",
    "        self.flow_norm, self.flow_data = self.pre_process_data(\n",
    "            data=get_flow_data(data_path[1]), norm_dim=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train_mode == \"train\":\n",
    "            # return self.train_days * self.one_day_length - self.history_length  # 这里为什么要减掉一个history length，导致最后训练数据的长度为45*24*12-6=12954\n",
    "            return self.train_days * self.one_day_length  # 这里为什么要减掉一个history length，导致最后训练数据的长度为45*24*12-6=12954\n",
    "        elif self.train_mode == \"test\":\n",
    "            return self.test_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(\n",
    "                self.train_mode))\n",
    "\n",
    "    def __getitem__(self, index):  # (x, y), index = [0, L1 - 1]\n",
    "        if self.train_mode == \"train\":\n",
    "            index = index\n",
    "        elif self.train_mode == \"test\":\n",
    "            index += self.train_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(\n",
    "                self.train_mode))\n",
    "\n",
    "        data_x, data_y = PEMSDataset.slice_data(self.flow_data,\n",
    "                                                self.history_length, index,\n",
    "                                                self.train_mode)\n",
    "        data_x = PEMSDataset.to_tensor(data_x)  # [N, H, D]\n",
    "        data_y = PEMSDataset.to_tensor(data_y).unsqueeze(1)  # [N, 1, D]\n",
    "        return {\n",
    "            \"graph\": PEMSDataset.to_tensor(self.graph),\n",
    "            \"flow_x\": data_x,\n",
    "            \"flow_y\": data_y\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def slice_data(data, history_length, index, train_mode):\n",
    "        \"\"\"\n",
    "        :param data: np.array, normalized traffic data.\n",
    "        :param history_length: int, length of history data to be used.\n",
    "        :param index: int, index on temporal axis.\n",
    "        :param train_mode: str, [\"train\", \"test\"].\n",
    "        :return:\n",
    "            data_x: np.array, [N, H, D].\n",
    "            data_y: np.array [N, D].\n",
    "        \"\"\"\n",
    "        if train_mode == \"train\":\n",
    "            start_index = index\n",
    "            end_index = index + history_length\n",
    "        elif train_mode == \"test\":\n",
    "            start_index = index - history_length\n",
    "            end_index = index\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"train model {} is not defined\".format(train_mode))\n",
    "\n",
    "        data_x = data[:, start_index:end_index]\n",
    "        data_y = data[:, end_index]\n",
    "\n",
    "        return data_x, data_y\n",
    "\n",
    "    @staticmethod\n",
    "    def pre_process_data(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            norm_base: list, [max_data, min_data], data of normalization base.\n",
    "            norm_data: np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        norm_base = PEMSDataset.normalize_base(\n",
    "            data, norm_dim)  # find the normalize base\n",
    "        norm_data = PEMSDataset.normalize_data(norm_base[0], norm_base[1],\n",
    "                                               data)  # normalize data\n",
    "\n",
    "        return norm_base, norm_data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_base(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            max_data: np.array\n",
    "            min_data: np.array\n",
    "        \"\"\"\n",
    "        max_data = np.max(data, norm_dim,\n",
    "                          keepdims=True)  # [N, T, D] , norm_dim=1, [N, 1, D]\n",
    "        min_data = np.min(data, norm_dim, keepdims=True)\n",
    "        return max_data, min_data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :return:\n",
    "            np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "        normalized_data = (data - mid) / base\n",
    "\n",
    "        return normalized_data\n",
    "\n",
    "    @staticmethod\n",
    "    def recover_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, normalized data.\n",
    "        :return:\n",
    "            recovered_data: np.array, recovered data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "\n",
    "        recovered_data = data * base + mid\n",
    "\n",
    "        return recovered_data\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensor(data):\n",
    "        return torch.tensor(data, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow(filename):\n",
    "    flow_data = np.load(filename)\n",
    "    # print(type(flow_data))\n",
    "    # PEMS数据集的data.npz文件中的data这个文件存放数据\n",
    "    return flow_data['data']\n",
    "\n",
    "def read_dataset(dataset):\n",
    "    data = np.load(dataset)\n",
    "    print(type(data['data']))\n",
    "    print(data['data'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.PEMSDataset'>\n",
      "12960\n",
      "<class 'dict'>\n",
      "torch.Size([307, 307])\n",
      "torch.Size([307, 6, 1])\n",
      "torch.Size([307, 1, 1])\n",
      "4032\n"
     ]
    }
   ],
   "source": [
    "def get_loader(ds_name=\"PEMS04\"):\n",
    "    num_nodes = 307 if ds_name == 'PEMS04' else 170\n",
    "    train_data = PEMSDataset(data_path=[\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/distance.csv\".format(ds_name),\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/data.npz\".format(ds_name)\n",
    "    ],\n",
    "                             num_nodes=num_nodes,\n",
    "                             divide_days=[45, 14],\n",
    "                             time_interval=5,\n",
    "                             history_length=6,\n",
    "                             train_mode=\"train\")\n",
    "\n",
    "    print(type(train_data))\n",
    "    print(len(train_data))\n",
    "    print(type(train_data[0]))\n",
    "    print(train_data[0]['graph'].shape)\n",
    "    print(train_data[0]['flow_x'].shape)\n",
    "    print(train_data[0]['flow_y'].shape)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_data = PEMSDataset(data_path=[\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/distance.csv\".format(ds_name),\n",
    "        \"/home/zhoujianping/Research/GNN/TrafficPrediction/datasets/{}/data.npz\".format(ds_name)\n",
    "    ],\n",
    "                            num_nodes=num_nodes,\n",
    "                            divide_days=[45, 14],\n",
    "                            time_interval=5,\n",
    "                            history_length=6,\n",
    "                            train_mode=\"test\")\n",
    "\n",
    "    print(len(test_data))\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "    \n",
    "train_loader, test_loader = get_loader('PEMS04')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 307, 6, 1])\n",
      "torch.Size([64, 307, 6])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data['flow_x'].shape)\n",
    "    print(torch.squeeze(data['flow_x'],dim=3).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "# Hyper-parameters\n",
    "image_size = 1842    # mnist数据集中一张图片的size，28*28\n",
    "h_dim = 400 \n",
    "z_dim = 20  \n",
    "num_epochs = 10 # 迭代次数\n",
    "batch_size = 64    # 一批样本的数量\n",
    "learning_rate = 1e-3    # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size=1842, h_dim=400, z_dim=20):\n",
    "        # 调用父类方法初始化模块的state\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # 编码器： [b,input_dim]-->[b,z_dim]\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)  # 均值 向量\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)  # 保准方差 向量\n",
    "\n",
    "        # 解码器：[b,z_dim]-->[b,input_dim]\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "\n",
    "    # 编码过程\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        encoding part\n",
    "        :param x: input image\n",
    "        :return: mu and log_var\n",
    "        \"\"\"\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc2(h)\n",
    "        log_var = self.fc3(h)\n",
    "        # return self.fc2(h), self.fc3(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    # 随机生成隐含向量\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Given a standard gaussian distribution epsilon ~ N(0,1),\n",
    "        we can sample the random variable z as per z = mu + sigma * epsilon\n",
    "        :param mu:\n",
    "        :param log_var:\n",
    "        :return: sampled z\n",
    "        \"\"\"\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    # 解码过程\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Given a sampled z, decode it back to image\n",
    "        :param z:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        h = F.relu(self.fc4(z))\n",
    "        x_reconst = F.sigmoid(self.fc5(h))\n",
    "        # return F.sigmoid(self.fc5(h))\n",
    "        return x_reconst\n",
    "\n",
    "    # 整个前向传播过程：编码-》解码\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        向前传播部分, 在model_name(inputs)时自动调用\n",
    "        :param x: the input of our training model [b, batch_size, 1, 28, 28]\n",
    "        :return: the result of our training model\n",
    "        \"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设备配置\n",
    "torch.cuda.set_device(0) # 这句用来设置pytorch在哪块GPU上运行\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个模型\n",
    "model = VAE().to(device)\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/10], Step [100/203], Reconst Loss: 61268.6562, KL Div: 758.0039\n",
      "Epoch[1/10], Step [200/203], Reconst Loss: 64866.1016, KL Div: 642.5427\n",
      "Epoch[2/10], Step [100/203], Reconst Loss: 63678.6523, KL Div: 611.8344\n",
      "Epoch[2/10], Step [200/203], Reconst Loss: 60216.4297, KL Div: 580.6030\n",
      "Epoch[3/10], Step [100/203], Reconst Loss: 61349.4297, KL Div: 567.4918\n",
      "Epoch[3/10], Step [200/203], Reconst Loss: 66332.0859, KL Div: 517.0936\n",
      "Epoch[4/10], Step [100/203], Reconst Loss: 62164.1797, KL Div: 497.1253\n",
      "Epoch[4/10], Step [200/203], Reconst Loss: 61858.0938, KL Div: 488.9102\n",
      "Epoch[5/10], Step [100/203], Reconst Loss: 61873.6484, KL Div: 472.7031\n",
      "Epoch[5/10], Step [200/203], Reconst Loss: 62857.2656, KL Div: 466.8595\n",
      "Epoch[6/10], Step [100/203], Reconst Loss: 62506.0000, KL Div: 472.3004\n",
      "Epoch[6/10], Step [200/203], Reconst Loss: 64931.8047, KL Div: 476.4633\n",
      "Epoch[7/10], Step [100/203], Reconst Loss: 62782.0977, KL Div: 463.4772\n",
      "Epoch[7/10], Step [200/203], Reconst Loss: 64430.5430, KL Div: 469.3087\n",
      "Epoch[8/10], Step [100/203], Reconst Loss: 63761.7305, KL Div: 453.9801\n",
      "Epoch[8/10], Step [200/203], Reconst Loss: 62180.7305, KL Div: 448.3519\n",
      "Epoch[9/10], Step [100/203], Reconst Loss: 65906.6875, KL Div: 460.3257\n",
      "Epoch[9/10], Step [200/203], Reconst Loss: 60768.0156, KL Div: 461.0601\n",
      "Epoch[10/10], Step [100/203], Reconst Loss: 63384.2578, KL Div: 466.5727\n",
      "Epoch[10/10], Step [200/203], Reconst Loss: 60017.5586, KL Div: 452.6406\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # 获取样本，并前向传播\n",
    "        x=torch.squeeze(data['flow_x'],dim=3)\n",
    "        x = x.to(device).view(-1, image_size)   # 转换成二维（128,784）\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "\n",
    "        # 计算重构损失和KL散度（KL散度用于衡量两种分布的相似程度）\n",
    "        # KL散度的计算可以参考论文或者文章开头的链接\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                \"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\"\n",
    "                .format(epoch + 1, num_epochs, i + 1, len(train_loader),\n",
    "                        reconst_loss.item(), kl_div.item()))\n",
    "    # print(x_reconst.shape)  # VAE输出的vector维度是(96,784)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be6a42a38cf19a2646c3ff61fbe6f9cf32a117da641e0e6e146e744a0b5291c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
